{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cf19ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchsummary\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "\n",
    "# torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc5d9cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdongju_coredottoday\u001b[0m (\u001b[33mcoredottoday-dongju\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7acc8f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagefloder 생성\n",
    "train_imgfolder = ImageFolder(root='data/train',\n",
    "                      transform=transforms.Compose([\n",
    "                          transforms.ToTensor(),\n",
    "                          transforms.Resize(224),\n",
    "                          transforms.RandomHorizontalFlip(p=0.8),\n",
    "#                           transforms.GaussianBlur(kernel_size=(19, 19), sigma=(1.0, 2.0)),\n",
    "                          transforms.RandomRotation(degrees=(-30, 30), interpolation=transforms.InterpolationMode.BILINEAR, fill=0),\n",
    "                          transforms.CenterCrop(224)\n",
    "                      ]))\n",
    "\n",
    "\n",
    "validation_imgfolder = ImageFolder(root='data/validation',\n",
    "                          transform=transforms.Compose([\n",
    "                          transforms.ToTensor(),\n",
    "                          transforms.Resize(224),\n",
    "                          transforms.CenterCrop(224)\n",
    "                      ]))\n",
    "\n",
    "\n",
    "test_imgfolder = ImageFolder(root='data/test',\n",
    "                      transform=transforms.Compose([\n",
    "                          transforms.ToTensor(),\n",
    "                          transforms.Resize(224),\n",
    "                          transforms.CenterCrop(224)\n",
    "                      ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "209ca9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data_loader\n",
    "train_data_loader = DataLoader(dataset=train_imgfolder, \n",
    "                         batch_size=16, \n",
    "                         num_workers=0,\n",
    "                         shuffle=True,\n",
    "                         drop_last=True\n",
    "                        )\n",
    "\n",
    "valid_data_loader = DataLoader(dataset=validation_imgfolder, \n",
    "                         batch_size=16, \n",
    "                         num_workers=0\n",
    "                        )\n",
    "\n",
    "test_data_loader = DataLoader(dataset=test_imgfolder, \n",
    "                         batch_size=1, \n",
    "                         num_workers=0\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3418fdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer9 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer10 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer11 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer12 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer13 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            # nn.Dropout(0.5),\n",
    "            nn.Linear(7*7*512, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            # nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU())\n",
    "        self.fc2= nn.Sequential(\n",
    "            nn.Linear(1024, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = self.layer7(out)\n",
    "        out = self.layer8(out)\n",
    "        out = self.layer9(out)\n",
    "        out = self.layer10(out)\n",
    "        out = self.layer11(out)\n",
    "        out = self.layer12(out)\n",
    "        out = self.layer13(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15cfee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        torch.nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.constant_(m.weight, 1)\n",
    "        torch.nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e8f1b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4\n",
    "num_epochs = 100\n",
    "batch_size = 16\n",
    "learning_rate = 0.0001\n",
    "\n",
    "model = VGG16(num_classes).to(device)\n",
    "model.apply(normal_init)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = 0.0001)  \n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b22375c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
      "              ReLU-3         [-1, 64, 224, 224]               0\n",
      "            Conv2d-4         [-1, 64, 224, 224]          36,928\n",
      "       BatchNorm2d-5         [-1, 64, 224, 224]             128\n",
      "              ReLU-6         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-7         [-1, 64, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]          73,856\n",
      "       BatchNorm2d-9        [-1, 128, 112, 112]             256\n",
      "             ReLU-10        [-1, 128, 112, 112]               0\n",
      "           Conv2d-11        [-1, 128, 112, 112]         147,584\n",
      "      BatchNorm2d-12        [-1, 128, 112, 112]             256\n",
      "             ReLU-13        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-14          [-1, 128, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         295,168\n",
      "      BatchNorm2d-16          [-1, 256, 56, 56]             512\n",
      "             ReLU-17          [-1, 256, 56, 56]               0\n",
      "           Conv2d-18          [-1, 256, 56, 56]         590,080\n",
      "      BatchNorm2d-19          [-1, 256, 56, 56]             512\n",
      "             ReLU-20          [-1, 256, 56, 56]               0\n",
      "           Conv2d-21          [-1, 256, 56, 56]         590,080\n",
      "      BatchNorm2d-22          [-1, 256, 56, 56]             512\n",
      "             ReLU-23          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-24          [-1, 256, 28, 28]               0\n",
      "           Conv2d-25          [-1, 512, 28, 28]       1,180,160\n",
      "      BatchNorm2d-26          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-27          [-1, 512, 28, 28]               0\n",
      "           Conv2d-28          [-1, 512, 28, 28]       2,359,808\n",
      "      BatchNorm2d-29          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-30          [-1, 512, 28, 28]               0\n",
      "           Conv2d-31          [-1, 512, 28, 28]       2,359,808\n",
      "      BatchNorm2d-32          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-33          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-34          [-1, 512, 14, 14]               0\n",
      "           Conv2d-35          [-1, 512, 14, 14]       2,359,808\n",
      "      BatchNorm2d-36          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-37          [-1, 512, 14, 14]               0\n",
      "           Conv2d-38          [-1, 512, 14, 14]       2,359,808\n",
      "      BatchNorm2d-39          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-40          [-1, 512, 14, 14]               0\n",
      "           Conv2d-41          [-1, 512, 14, 14]       2,359,808\n",
      "      BatchNorm2d-42          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-43          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-44            [-1, 512, 7, 7]               0\n",
      "           Linear-45                 [-1, 4096]     102,764,544\n",
      "             ReLU-46                 [-1, 4096]               0\n",
      "           Linear-47                 [-1, 1024]       4,195,328\n",
      "             ReLU-48                 [-1, 1024]               0\n",
      "           Linear-49                    [-1, 4]           4,100\n",
      "================================================================\n",
      "Total params: 121,687,108\n",
      "Trainable params: 121,687,108\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 321.83\n",
      "Params size (MB): 464.20\n",
      "Estimated Total Size (MB): 786.61\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torchsummary.summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb90d9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project=\"pbl\",\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"eprchs\": num_epochs\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3c53790",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datanum = len(train_data_loader)*batch_size\n",
    "valid_datanum = len(valid_data_loader)*batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a445f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Epoch [1/100] Loss: 0.8198 Accuracy: 0.6954\n",
      "Epoch [1/100] val_Loss: 0.6257 val_Accuracy: 0.7812\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [2/100] Loss: 0.5265 Accuracy: 0.8111\n",
      "Epoch [2/100] val_Loss: 0.4991 val_Accuracy: 0.8906\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [3/100] Loss: 0.4254 Accuracy: 0.8470\n",
      "Epoch [3/100] val_Loss: 1.9831 val_Accuracy: 0.6406\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [4/100] Loss: 0.4406 Accuracy: 0.8484\n",
      "Epoch [4/100] val_Loss: 0.3328 val_Accuracy: 0.9219\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [5/100] Loss: 0.3707 Accuracy: 0.8764\n",
      "Epoch [5/100] val_Loss: 0.4465 val_Accuracy: 0.8438\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [6/100] Loss: 0.3476 Accuracy: 0.8800\n",
      "Epoch [6/100] val_Loss: 0.3603 val_Accuracy: 0.8906\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [7/100] Loss: 0.3609 Accuracy: 0.8671\n",
      "Epoch [7/100] val_Loss: 0.6172 val_Accuracy: 0.8750\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [8/100] Loss: 0.3092 Accuracy: 0.8865\n",
      "Epoch [8/100] val_Loss: 0.2652 val_Accuracy: 0.9062\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [9/100] Loss: 0.2826 Accuracy: 0.9009\n",
      "Epoch [9/100] val_Loss: 0.4422 val_Accuracy: 0.8438\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [10/100] Loss: 0.3029 Accuracy: 0.8937\n",
      "Epoch [10/100] val_Loss: 0.3531 val_Accuracy: 0.8281\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [11/100] Loss: 0.2808 Accuracy: 0.9095\n",
      "Epoch [11/100] val_Loss: 0.6490 val_Accuracy: 0.8125\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [12/100] Loss: 0.2532 Accuracy: 0.9124\n",
      "Epoch [12/100] val_Loss: 0.3424 val_Accuracy: 0.8906\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [13/100] Loss: 0.2737 Accuracy: 0.9052\n",
      "Epoch [13/100] val_Loss: 0.5609 val_Accuracy: 0.8125\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [14/100] Loss: 0.2663 Accuracy: 0.9102\n",
      "Epoch [14/100] val_Loss: 0.2974 val_Accuracy: 0.8906\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [15/100] Loss: 0.2460 Accuracy: 0.9167\n",
      "Epoch [15/100] val_Loss: 0.3890 val_Accuracy: 0.8438\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [16/100] Loss: 0.2342 Accuracy: 0.9167\n",
      "Epoch [16/100] val_Loss: 0.4589 val_Accuracy: 0.8125\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [17/100] Loss: 0.2127 Accuracy: 0.9282\n",
      "Epoch [17/100] val_Loss: 0.4331 val_Accuracy: 0.7969\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [18/100] Loss: 0.2508 Accuracy: 0.9124\n",
      "Epoch [18/100] val_Loss: 0.3263 val_Accuracy: 0.8750\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [19/100] Loss: 0.2147 Accuracy: 0.9318\n",
      "Epoch [19/100] val_Loss: 0.3029 val_Accuracy: 0.9062\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [20/100] Loss: 0.2037 Accuracy: 0.9224\n",
      "Epoch [20/100] val_Loss: 0.3648 val_Accuracy: 0.8438\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [21/100] Loss: 0.2141 Accuracy: 0.9224\n",
      "Epoch [21/100] val_Loss: 0.3378 val_Accuracy: 0.8906\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [22/100] Loss: 0.1852 Accuracy: 0.9303\n",
      "Epoch [22/100] val_Loss: 0.4982 val_Accuracy: 0.7656\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [23/100] Loss: 0.2027 Accuracy: 0.9303\n",
      "Epoch [23/100] val_Loss: 0.2817 val_Accuracy: 0.8906\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [24/100] Loss: 0.1907 Accuracy: 0.9339\n",
      "Epoch [24/100] val_Loss: 0.3449 val_Accuracy: 0.8906\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [25/100] Loss: 0.1735 Accuracy: 0.9461\n",
      "Epoch [25/100] val_Loss: 0.2694 val_Accuracy: 0.9062\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [26/100] Loss: 0.1694 Accuracy: 0.9361\n",
      "Epoch [26/100] val_Loss: 0.4856 val_Accuracy: 0.8906\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [27/100] Loss: 0.1707 Accuracy: 0.9425\n",
      "Epoch [27/100] val_Loss: 0.2971 val_Accuracy: 0.9062\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [28/100] Loss: 0.1426 Accuracy: 0.9490\n",
      "Epoch [28/100] val_Loss: 0.3130 val_Accuracy: 0.8906\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [29/100] Loss: 0.1729 Accuracy: 0.9368\n",
      "Epoch [29/100] val_Loss: 0.2190 val_Accuracy: 0.9062\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [30/100] Loss: 0.1748 Accuracy: 0.9404\n",
      "Epoch [30/100] val_Loss: 0.3440 val_Accuracy: 0.9062\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [31/100] Loss: 0.1478 Accuracy: 0.9533\n",
      "Epoch [31/100] val_Loss: 0.6796 val_Accuracy: 0.7500\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [32/100] Loss: 0.1223 Accuracy: 0.9634\n",
      "Epoch [32/100] val_Loss: 0.4660 val_Accuracy: 0.8594\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [33/100] Loss: 0.1238 Accuracy: 0.9547\n",
      "Epoch [33/100] val_Loss: 0.1728 val_Accuracy: 0.9219\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [34/100] Loss: 0.1279 Accuracy: 0.9605\n",
      "Epoch [34/100] val_Loss: 0.1992 val_Accuracy: 0.9375\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [35/100] Loss: 0.1014 Accuracy: 0.9648\n",
      "Epoch [35/100] val_Loss: 0.3125 val_Accuracy: 0.9375\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [36/100] Loss: 0.1236 Accuracy: 0.9598\n",
      "Epoch [36/100] val_Loss: 0.1899 val_Accuracy: 0.9219\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [37/100] Loss: 0.1198 Accuracy: 0.9591\n",
      "Epoch [37/100] val_Loss: 0.3423 val_Accuracy: 0.9219\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [38/100] Loss: 0.1239 Accuracy: 0.9583\n",
      "Epoch [38/100] val_Loss: 0.2104 val_Accuracy: 0.9219\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [39/100] Loss: 0.1366 Accuracy: 0.9511\n",
      "Epoch [39/100] val_Loss: 0.1976 val_Accuracy: 0.9375\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [40/100] Loss: 0.0966 Accuracy: 0.9662\n",
      "Epoch [40/100] val_Loss: 0.2323 val_Accuracy: 0.9375\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [41/100] Loss: 0.1008 Accuracy: 0.9655\n",
      "Epoch [41/100] val_Loss: 0.3220 val_Accuracy: 0.8906\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [42/100] Loss: 0.1090 Accuracy: 0.9655\n",
      "Epoch [42/100] val_Loss: 0.2700 val_Accuracy: 0.9375\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [43/100] Loss: 0.0995 Accuracy: 0.9662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/100] val_Loss: 0.5567 val_Accuracy: 0.9062\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [44/100] Loss: 0.0961 Accuracy: 0.9655\n",
      "Epoch [44/100] val_Loss: 0.1634 val_Accuracy: 0.9375\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [45/100] Loss: 0.1062 Accuracy: 0.9591\n",
      "Epoch [45/100] val_Loss: 0.2077 val_Accuracy: 0.9219\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [46/100] Loss: 0.1005 Accuracy: 0.9691\n",
      "Epoch [46/100] val_Loss: 0.3107 val_Accuracy: 0.9219\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [47/100] Loss: 0.0861 Accuracy: 0.9677\n",
      "Epoch [47/100] val_Loss: 0.3315 val_Accuracy: 0.9062\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [48/100] Loss: 0.0936 Accuracy: 0.9662\n",
      "Epoch [48/100] val_Loss: 0.4737 val_Accuracy: 0.8281\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [49/100] Loss: 0.0985 Accuracy: 0.9684\n",
      "Epoch [49/100] val_Loss: 0.3499 val_Accuracy: 0.8750\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [50/100] Loss: 0.0772 Accuracy: 0.9756\n",
      "Epoch [50/100] val_Loss: 0.2358 val_Accuracy: 0.9531\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [51/100] Loss: 0.1300 Accuracy: 0.9483\n",
      "Epoch [51/100] val_Loss: 0.1877 val_Accuracy: 0.9219\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [52/100] Loss: 0.0893 Accuracy: 0.9677\n",
      "Epoch [52/100] val_Loss: 0.4891 val_Accuracy: 0.8594\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [53/100] Loss: 0.1184 Accuracy: 0.9598\n",
      "Epoch [53/100] val_Loss: 0.4154 val_Accuracy: 0.8125\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [54/100] Loss: 0.0801 Accuracy: 0.9720\n",
      "Epoch [54/100] val_Loss: 0.3764 val_Accuracy: 0.8281\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [55/100] Loss: 0.0502 Accuracy: 0.9792\n",
      "Epoch [55/100] val_Loss: 0.2658 val_Accuracy: 0.9062\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [56/100] Loss: 0.0927 Accuracy: 0.9670\n",
      "Epoch [56/100] val_Loss: 0.1857 val_Accuracy: 0.9375\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [57/100] Loss: 0.0849 Accuracy: 0.9691\n",
      "Epoch [57/100] val_Loss: 0.2042 val_Accuracy: 0.9531\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [58/100] Loss: 0.0748 Accuracy: 0.9727\n",
      "Epoch [58/100] val_Loss: 0.6552 val_Accuracy: 0.7344\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [59/100] Loss: 0.0636 Accuracy: 0.9784\n",
      "Epoch [59/100] val_Loss: 0.2610 val_Accuracy: 0.9062\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [60/100] Loss: 0.0808 Accuracy: 0.9713\n",
      "Epoch [60/100] val_Loss: 0.4029 val_Accuracy: 0.8906\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [61/100] Loss: 0.0890 Accuracy: 0.9684\n",
      "Epoch [61/100] val_Loss: 0.3252 val_Accuracy: 0.8594\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [62/100] Loss: 0.0650 Accuracy: 0.9777\n",
      "Epoch [62/100] val_Loss: 0.2722 val_Accuracy: 0.9062\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [63/100] Loss: 0.0772 Accuracy: 0.9713\n",
      "Epoch [63/100] val_Loss: 0.3362 val_Accuracy: 0.8906\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [64/100] Loss: 0.0793 Accuracy: 0.9734\n",
      "Epoch [64/100] val_Loss: 0.2701 val_Accuracy: 0.9219\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [65/100] Loss: 0.0790 Accuracy: 0.9727\n",
      "Epoch [65/100] val_Loss: 0.3394 val_Accuracy: 0.8906\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [66/100] Loss: 0.0751 Accuracy: 0.9734\n",
      "Epoch [66/100] val_Loss: 0.3078 val_Accuracy: 0.9062\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [67/100] Loss: 0.0669 Accuracy: 0.9756\n",
      "Epoch [67/100] val_Loss: 0.1411 val_Accuracy: 0.9375\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [68/100] Loss: 0.0391 Accuracy: 0.9892\n",
      "Epoch [68/100] val_Loss: 0.2688 val_Accuracy: 0.9375\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [69/100] Loss: 0.0785 Accuracy: 0.9770\n",
      "Epoch [69/100] val_Loss: 0.2033 val_Accuracy: 0.9219\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [70/100] Loss: 0.0514 Accuracy: 0.9799\n",
      "Epoch [70/100] val_Loss: 0.3843 val_Accuracy: 0.9062\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [71/100] Loss: 0.0525 Accuracy: 0.9820\n",
      "Epoch [71/100] val_Loss: 0.1346 val_Accuracy: 0.9375\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [72/100] Loss: 0.0994 Accuracy: 0.9641\n",
      "Epoch [72/100] val_Loss: 0.1662 val_Accuracy: 0.9375\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [73/100] Loss: 0.0611 Accuracy: 0.9770\n",
      "Epoch [73/100] val_Loss: 0.3031 val_Accuracy: 0.8906\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [74/100] Loss: 0.0503 Accuracy: 0.9828\n",
      "Epoch [74/100] val_Loss: 0.0764 val_Accuracy: 0.9375\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [75/100] Loss: 0.0502 Accuracy: 0.9864\n",
      "Epoch [75/100] val_Loss: 0.2017 val_Accuracy: 0.9375\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [76/100] Loss: 0.0683 Accuracy: 0.9763\n",
      "Epoch [76/100] val_Loss: 0.1618 val_Accuracy: 0.9219\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [77/100] Loss: 0.0747 Accuracy: 0.9727\n",
      "Epoch [77/100] val_Loss: 0.1055 val_Accuracy: 0.9375\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [78/100] Loss: 0.0759 Accuracy: 0.9763\n",
      "Epoch [78/100] val_Loss: 0.2579 val_Accuracy: 0.9219\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [79/100] Loss: 0.0724 Accuracy: 0.9756\n",
      "Epoch [79/100] val_Loss: 0.1695 val_Accuracy: 0.9219\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [80/100] Loss: 0.0581 Accuracy: 0.9806\n",
      "Epoch [80/100] val_Loss: 0.2113 val_Accuracy: 0.9375\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [81/100] Loss: 0.0520 Accuracy: 0.9828\n",
      "Epoch [81/100] val_Loss: 0.1767 val_Accuracy: 0.9375\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [82/100] Loss: 0.0388 Accuracy: 0.9864\n",
      "Epoch [82/100] val_Loss: 0.2551 val_Accuracy: 0.9375\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [83/100] Loss: 0.0549 Accuracy: 0.9842\n",
      "Epoch [83/100] val_Loss: 0.2212 val_Accuracy: 0.9375\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [84/100] Loss: 0.0580 Accuracy: 0.9820\n",
      "Epoch [84/100] val_Loss: 0.2094 val_Accuracy: 0.9375\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [85/100] Loss: 0.0508 Accuracy: 0.9820\n",
      "Epoch [85/100] val_Loss: 0.2339 val_Accuracy: 0.9219\n",
      "-----------------next-----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Epoch [86/100] Loss: 0.0607 Accuracy: 0.9792\n",
      "Epoch [86/100] val_Loss: 0.3435 val_Accuracy: 0.9219\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [87/100] Loss: 0.0607 Accuracy: 0.9784\n",
      "Epoch [87/100] val_Loss: 0.1838 val_Accuracy: 0.9062\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [88/100] Loss: 0.0381 Accuracy: 0.9864\n",
      "Epoch [88/100] val_Loss: 0.2318 val_Accuracy: 0.9375\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [89/100] Loss: 0.0549 Accuracy: 0.9806\n",
      "Epoch [89/100] val_Loss: 0.2702 val_Accuracy: 0.8750\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [90/100] Loss: 0.0676 Accuracy: 0.9741\n",
      "Epoch [90/100] val_Loss: 0.1477 val_Accuracy: 0.9531\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [91/100] Loss: 0.0411 Accuracy: 0.9849\n",
      "Epoch [91/100] val_Loss: 0.2237 val_Accuracy: 0.9219\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [92/100] Loss: 0.0378 Accuracy: 0.9907\n",
      "Epoch [92/100] val_Loss: 0.2295 val_Accuracy: 0.9375\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [93/100] Loss: 0.0581 Accuracy: 0.9806\n",
      "Epoch [93/100] val_Loss: 0.1264 val_Accuracy: 0.9375\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [94/100] Loss: 0.0600 Accuracy: 0.9777\n",
      "Epoch [94/100] val_Loss: 0.1566 val_Accuracy: 0.9531\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [95/100] Loss: 0.0358 Accuracy: 0.9878\n",
      "Epoch [95/100] val_Loss: 0.1512 val_Accuracy: 0.9375\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [96/100] Loss: 0.0601 Accuracy: 0.9741\n",
      "Epoch [96/100] val_Loss: 0.1169 val_Accuracy: 0.9531\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [97/100] Loss: 0.0432 Accuracy: 0.9842\n",
      "Epoch [97/100] val_Loss: 0.3655 val_Accuracy: 0.9219\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [98/100] Loss: 0.0468 Accuracy: 0.9820\n",
      "Epoch [98/100] val_Loss: 0.2701 val_Accuracy: 0.9062\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [99/100] Loss: 0.0348 Accuracy: 0.9885\n",
      "Epoch [99/100] val_Loss: 0.2076 val_Accuracy: 0.9219\n",
      "-----------------next-----------------\n",
      "--------------------------------------------------------\n",
      "Epoch [100/100] Loss: 0.0714 Accuracy: 0.9720\n",
      "Epoch [100/100] val_Loss: 0.2387 val_Accuracy: 0.9375\n",
      "-----------------next-----------------\n"
     ]
    }
   ],
   "source": [
    "# 학습 진행\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_sum = 0\n",
    "    correct = 0\n",
    "    model.train()\n",
    "    for train_x, train_y in train_data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x.cuda())\n",
    "        \n",
    "        loss = criterion(output.cpu(), train_y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        \n",
    "\n",
    "\n",
    "        predicted = torch.max(output, 1)[1]        \n",
    "        correct += (train_y == predicted.cpu()).sum()\n",
    "\n",
    "    print(f'--------------------------------------------------------')\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] Loss: {loss_sum/len(train_data_loader):.4f} Accuracy: {correct / train_datanum:.4f}')\n",
    "    train_loss = loss_sum/len(train_data_loader)\n",
    "    train_acc = correct / train_datanum\n",
    "    \n",
    "    val_loss_sum = 0\n",
    "    val_correct = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for valid_x, valid_y in valid_data_loader:\n",
    "            valid_x = valid_x.to('cuda')\n",
    "\n",
    "            val_output = model(valid_x)\n",
    "            val_loss = criterion(val_output.cpu(), valid_y)\n",
    "            val_loss_sum += val_loss.item()\n",
    "            \n",
    "            val_predicted = torch.max(val_output, 1)[1]\n",
    "            val_correct += (valid_y == val_predicted.cpu()).sum()\n",
    "    \n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] val_Loss: {val_loss_sum/len(valid_data_loader):.4f} val_Accuracy: {val_correct / valid_datanum:.4f}')\n",
    "\n",
    "    validation_loss = val_loss_sum/len(valid_data_loader)\n",
    "    validation_acc = val_correct / valid_datanum\n",
    "    \n",
    "    \n",
    "    wandb.log({\"train_acc\":train_acc,\"train_loss\":train_loss,\"validation_acc\":validation_acc,\"validation_loss\":validation_loss})\n",
    "        \n",
    "    print('-----------------next-----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a0fa362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "torch.save(model.state_dict(), 'VGG_custom.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfb9c3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 불러오기\n",
    "test_model = VGG16(num_classes)\n",
    "test_model.load_state_dict(torch.load('VGG_custom.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aabdb512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기화\n",
    "test_loss = 0\n",
    "test_correct = 0\n",
    "test_loss_sum = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for test_x, test_y in test_data_loader:\n",
    "        test_x = test_x.to('cuda')\n",
    "        test_y = test_y.to('cuda')\n",
    "\n",
    "        test_output = test_model.to('cuda')(test_x)\n",
    "        test_loss = criterion(test_output, test_y)\n",
    "        test_loss_sum += test_loss.item()\n",
    "        predicted = torch.max(test_output, 1)[1]\n",
    "        test_correct += (test_y == predicted).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08654b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.21875\n"
     ]
    }
   ],
   "source": [
    "acc = test_correct / len(test_data_loader)\n",
    "print(f'Test Accuracy: {acc.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
